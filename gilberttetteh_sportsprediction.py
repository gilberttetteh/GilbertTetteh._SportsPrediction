# -*- coding: utf-8 -*-
"""GilbertTetteh_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qHkF9KcNMZ9f3eVKKxxwlXhvTUhpw__N
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBRegressor
from google.colab import drive
import pickle as pkl
import joblib
drive.mount('/content/drive')
#import streamlit as st

#loading the datasets
legacy = pd.read_csv('legacy.csv',na_values = "")
play_22 = pd.read_csv('players_22.csv')

legacy
play_22

#picking relevant reafutures
corrcolumns = ['overall', 'potential', 'age', 'height_cm', 'weight_kg', 'pace', 'shooting', 'passing',
                     'dribbling', 'defending', 'physic', 'attacking_crossing', 'attacking_finishing',
                     'attacking_heading_accuracy', 'attacking_short_passing', 'attacking_volleys',
                     'skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing',
                     'skill_ball_control', 'movement_acceleration', 'movement_sprint_speed', 'movement_agility',
                     'movement_reactions', 'movement_balance', 'power_shot_power', 'power_jumping', 'power_stamina',
                     'power_strength', 'power_long_shots', 'mentality_aggression', 'mentality_interceptions',
                     'mentality_positioning', 'mentality_vision', 'mentality_penalties', 'mentality_composure',
                     'defending_marking_awareness', 'defending_standing_tackle', 'defending_sliding_tackle',
                     'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning',
                     'goalkeeping_reflexes', 'goalkeeping_speed']

# Calculate correlation with 'overall' and sort
corrs= legacy[corrcolumns].corr()

highcorrs = corrs['overall'].sort_values(ascending=False)
highcorrs

# selecting top 30 features
corrcolumns = highcorrs.index[1:50].tolist()
corrcolumns.append('overall')
corrcolumns

legacy= pd.read_csv('legacy.csv', usecols=corrcolumns)

# filling missing values
legacy.fillna(0, inplace=True)

#making overall of 92 and 93 into 92
legacy['overall'] = legacy['overall'].apply(lambda x: 92 if x in [92, 93] else x)

# Shooting
shooting_attributes = ['shooting', 'power_shot_power', 'power_long_shots', 'attacking_volleys', 'attacking_finishing']
legacy['shooting_skills'] = legacy[shooting_attributes].mean(axis=1)
legacy.drop(columns=shooting_attributes, inplace=True)
missing_attributes = [attr for attr in shooting_attributes if attr not in legacy.columns]
if missing_attributes:
    print(f"Warning: The following shooting attributes are missing: {missing_attributes}")
else:
    legacy['shooting_skills'] = legacy[shooting_attributes].mean(axis=1)
    legacy.drop(columns=shooting_attributes, inplace=True)

#Mentality
mentality_attributes = ['mentality_aggression', 'mentality_interceptions', 'mentality_positioning',
                        'mentality_vision', 'mentality_penalties', 'mentality_composure']
imputer = SimpleImputer(strategy='mean')
legacy[mentality_attributes] = imputer.fit_transform(legacy[mentality_attributes])
legacy['mentality'] = legacy[mentality_attributes].mean(axis=1)
legacy.drop(columns=mentality_attributes, inplace=True)

#Technicality
skill_attributes = ['skill_long_passing', 'skill_ball_control', 'skill_curve', 'skill_fk_accuracy', 'skill_dribbling']
legacy['technical_skills'] = legacy[skill_attributes].mean(axis=1)
legacy.drop(columns=skill_attributes, inplace=True)

#Goalkeeping
goalkeeping_attributes = ['goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',
                          'goalkeeping_positioning', 'goalkeeping_reflexes', 'goalkeeping_speed']
legacy[goalkeeping_attributes] = imputer.fit_transform(legacy[goalkeeping_attributes])
legacy['goalkeeping_ability'] = legacy[goalkeeping_attributes].mean(axis=1)
legacy.drop(columns=goalkeeping_attributes, inplace=True)

#converting to integer type
legacy = legacy.astype(int)

#picking target variable and its features
y = legacy['overall']
X = legacy.drop(columns=['overall'])

# Standardizing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)

# Training and Evaluating
# Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
print(f'Mean Absolute Error for Random Forest: {mae_rf}')

# XGBoost Regressor
xgb_model = XGBRegressor()
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
print(f'Mean Absolute Error for XGBoost: {mae_xgb}')

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)
mae_gb = mean_absolute_error(y_test, y_pred_gb)
print(f'Mean Absolute Error for Gradient Boosting: {mae_gb}')

rf_params = {
    'n_estimators': [50,100],
    'max_depth': [5, 10],
    'min_samples_split': [2,10]
}
grid_rf = GridSearchCV(estimator=RandomForestRegressor(), param_grid=rf_params, scoring='neg_mean_absolute_error', cv=3)
grid_rf.fit(X_train, y_train)
best_rf = grid_rf.best_estimator_

# Hyperparameter tuning for XGBoost
xgb_params = {
    'n_estimators': [50,100],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001]
}
grid_xgb = GridSearchCV(estimator=XGBRegressor(), param_grid=xgb_params, scoring='neg_mean_absolute_error', cv=3)
grid_xgb.fit(X_train, y_train)
best_xgb = grid_xgb.best_estimator_

ensemble_model = VotingRegressor(estimators=[('rf', best_rf), ('xgb', best_xgb)])
ensemble_model.fit(X_train, y_train)
ensemble_predictions = ensemble_model.predict(X_test)
mae_ensemble = mean_absolute_error(y_test, ensemble_predictions)
print(f'Mean Absolute Error for Ensemble Model: {mae_ensemble}')

#saving scalar
joblib.dump(scaler, 'scaler_ensemble.pkl')

#saving ensembling model
filename = 'sports_prediction_ensemble_model.pkl'
pkl.dump(ensemble_model, open(filename, 'wb'))